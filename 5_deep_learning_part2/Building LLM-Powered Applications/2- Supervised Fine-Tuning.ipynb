{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367b3258",
   "metadata": {},
   "source": [
    "# Hugging Face: Supervised Fine-Tuning \n",
    "\n",
    "This notebook was created by Natasha Patnaik for MIT 15.S60 - Computing in Optimization and Statistics. \n",
    "\n",
    "**Last Updated**: January 2026.\n",
    "\n",
    "Although this code runs on a CPU, it will run much faster on an NVIDIA GPU using CUDA. In practice, you would typically access a GPU through a High-Performance Computing (HPC) cluster, such as MIT Engaging: https://orcd.mit.edu/resources/mit-campus-wide-resources. Alternatively, you can upload this notebook into Google Colab, adding appropriate `!pip install` lines to get the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58995fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\npatn\\OneDrive\\Documents\\15.S60_2026\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40592ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions for pedagogical purposes\n",
    "\n",
    "def describe_tensors(obj, name):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * len(name))\n",
    "\n",
    "    # Case 1: tokenizer output (BatchEncoding or dict)\n",
    "    if isinstance(obj, (dict, BatchEncoding)):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"{key:<15} shape = {tuple(value.shape)}\")\n",
    "            else:\n",
    "                print(f\"{key:<15} type  = {type(value)}\")\n",
    "\n",
    "    # Case 2: Hugging Face model outputs (ModelOutput dataclass)\n",
    "    elif isinstance(obj, ModelOutput):\n",
    "\n",
    "        for key, value in obj.__dict__.items():\n",
    "            if value is None:\n",
    "                continue\n",
    "\n",
    "            # Shape of individual tensor \n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"{key:<15} shape = {tuple(value.shape)}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"{key:<15} type  = {type(value)}\")\n",
    "\n",
    "    # Case 3: plain tensor\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        print(f\"Tensor shape = {tuple(obj.shape)}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unrecognized type: {type(obj)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc1733",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3433ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['token', '##ization', 'is', 'un', '##bel', '##ie', '##va', '##bly', 'important', 'for', 'language', 'models', '!']\n",
      "\n",
      "Token IDs:\n",
      "[19204, 3989, 2003, 4895, 8671, 2666, 3567, 6321, 2590, 2005, 2653, 4275, 999]\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT tokenizer\n",
    "# The first time from_pretrained() is called, the model is downloaded and cached locally. \n",
    "# On subsequent calls, the model is loaded from the local cache unless an update is available.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# BERT uses WordPiece tokenization.\n",
    "text = \"Tokenization is unbelievably important for language models!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\nToken IDs:\")\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e08c85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Inputs:\n",
      "------------\n",
      "input_ids       shape = (1, 9)\n",
      "token_type_ids  shape = (1, 9)\n",
      "attention_mask  shape = (1, 9)\n",
      "------------------------------\n",
      "Input token IDs: tensor([ 101, 3731, 2003, 1996, 3007, 1997,  103, 1012,  102])\n",
      "Input tokens: ['[CLS]', 'boston', 'is', 'the', 'capital', 'of', '[MASK]', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# The first time from_pretrained() is called, the model is downloaded and cached locally. \n",
    "# On subsequent calls, the model is loaded from the local cache unless an update is available.\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tokenize a simple masked sentence as input\n",
    "text = \"Boston is the capital of [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# inputs[\"input_ids\"]       = token indices into the WordPiece vocabulary\n",
    "# inputs[\"attention_mask\"]  = mask indicating which tokens should be attended to\n",
    "# inputs[\"token_type_ids\"]  = sentence/segment identifiers (used for sentence pairs)\n",
    "describe_tensors(inputs, \"Model Inputs\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Get the token IDs for the input sentence\n",
    "input_ids = inputs[\"input_ids\"][0] # first (and only) input sequence in the batch\n",
    "print(\"Input token IDs:\", input_ids)\n",
    "\n",
    "# Convert token IDs back to tokens (subwords)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(\"Input tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d8fd83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Outputs:\n",
      "-------------\n",
      "logits          shape = (1, 9, 30522)\n",
      "hidden_states   type  = <class 'tuple'>\n",
      "attentions      type  = <class 'tuple'>\n",
      "\n",
      "Example Hidden State Tensor:\n",
      "---------------------------\n",
      "Tensor shape = (1, 9, 768)\n",
      "\n",
      "Example Attention Tensor:\n",
      "------------------------\n",
      "Tensor shape = (1, 12, 9, 9)\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "# outputs.logits shape: (batch_size, sequence_length, vocab_size)\n",
    "#   batch_size      = number of input sequences processed at once\n",
    "#   sequence_length = number of tokens in each input (incl. [CLS], [SEP], [MASK])\n",
    "#   vocab_size      = size of BERT’s WordPiece vocabulary (approx. 30k tokens)\n",
    "\n",
    "# outputs.hidden_states: tuple of tensors, one per layer (+ embeddings)\n",
    "#   each tensor shape: (batch_size, sequence_length, hidden_size)\n",
    "#   for bert-base, hidden_size=768 means each token is represented by a 768-dimensional vector.\n",
    "\n",
    "# outputs.attentions: tuple of tensors, one per layer\n",
    "#   each tensor shape: (batch_size, num_heads, sequence_length, sequence_length)\n",
    "#   attention weights showing how each token attends to every other token\n",
    "\n",
    "describe_tensors(outputs, \"Model Outputs\")\n",
    "describe_tensors(outputs.hidden_states[0], \"Example Hidden State Tensor\")\n",
    "describe_tensors(outputs.attentions[0], \"Example Attention Tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758f1e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MASK] token ID: 103\n",
      "[MASK] token index in our input sentence: 6\n",
      "----------------------------------\n",
      "Predicted word: massachusetts\n"
     ]
    }
   ],
   "source": [
    "# Get the token ID that represents [MASK]\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "print(\"\\n[MASK] token ID:\", mask_token_id)\n",
    "\n",
    "# Find the position of the [MASK] token in the input sentence\n",
    "mask_positions = (input_ids == mask_token_id).nonzero(as_tuple=False)\n",
    "\n",
    "# Extract the token index \n",
    "mask_index = mask_positions.item()\n",
    "print(\"[MASK] token index in our input sentence:\", mask_index)\n",
    "\n",
    "# Get the model's prediction scores (logits) for the masked position\n",
    "mask_logits = outputs.logits[0, mask_index]\n",
    "\n",
    "# Choose the token ID with the highest score\n",
    "predicted_token_id = mask_logits.argmax(dim=-1)\n",
    "\n",
    "# Decode and print result\n",
    "predicted_word = tokenizer.decode(predicted_token_id)\n",
    "print(\"----------------------------------\")\n",
    "print(\"Predicted word:\", predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions for [MASK]:\n",
      "massachusetts -> 15.9126\n",
      "     maine -> 10.9696\n",
      "   america -> 10.8234\n",
      "    canada -> 10.4788\n",
      "   england -> 10.1370\n"
     ]
    }
   ],
   "source": [
    "# Get the top 5 tokens\n",
    "topk = torch.topk(mask_logits, k=5) # returns the top k values and their indices in the vocab\n",
    "topk_scores = topk.values.tolist()\n",
    "topk_ids = topk.indices.tolist()\n",
    "\n",
    "# Convert token IDs to strings\n",
    "topk_tokens = tokenizer.convert_ids_to_tokens(topk_ids)\n",
    "\n",
    "print(\"Top 5 predictions for [MASK]:\")\n",
    "for token, score in zip(topk_tokens, topk_scores):\n",
    "    print(f\"{token:>10} -> {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f55dc9",
   "metadata": {},
   "source": [
    "## Fine-tuning a Classification Model\n",
    "\n",
    "Let's try fine-tuning BERT on a *multi-label classification* task using google-research-datasets/go_emotions on Hugging Face. Here's the dataset card: https://huggingface.co/datasets/google-research-datasets/go_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c95e4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model context window: 512\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Global configuration\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATASET_NAME = \"google-research-datasets/go_emotions\"\n",
    "TEXT_COL = \"text\"\n",
    "\n",
    "# For classification, we only need to tokenize the input text.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "MAX_LENGTH =  tokenizer.model_max_length\n",
    "print(\"Model context window:\", MAX_LENGTH)\n",
    "\n",
    "# Classification threshold for converting probabilities to label predictions\n",
    "THRESHOLD = 0.5  \n",
    "\n",
    "# Sigmoid function to convert logits to probabilities\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Check whether we are using CUDA (GPU) or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a8db3",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f778cb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n",
      "--------------------------------------------------\n",
      "text: My favourite food is anything I didn't have to cook myself.\n",
      "labels: [27]\n",
      "id: eebbqej\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Dataset features schema ===\n",
      "text: Value('string')\n",
      "labels: List(ClassLabel(names=['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']))\n",
      "id: Value('string')\n",
      "--------------------------------------------------\n",
      "\n",
      "Labels column schema (list of IDs): List(ClassLabel(names=['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']))\n",
      "Label ID schema (ID to name mapping): ClassLabel(names=['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'])\n",
      "--------------------------------------------------\n",
      "\n",
      "Number of labels: 28\n",
      "Label names:\n",
      " 0 → admiration\n",
      " 1 → amusement\n",
      " 2 → anger\n",
      " 3 → annoyance\n",
      " 4 → approval\n",
      " 5 → caring\n",
      " 6 → confusion\n",
      " 7 → curiosity\n",
      " 8 → desire\n",
      " 9 → disappointment\n",
      "10 → disapproval\n",
      "11 → disgust\n",
      "12 → embarrassment\n",
      "13 → excitement\n",
      "14 → fear\n",
      "15 → gratitude\n",
      "16 → grief\n",
      "17 → joy\n",
      "18 → love\n",
      "19 → nervousness\n",
      "20 → optimism\n",
      "21 → pride\n",
      "22 → realization\n",
      "23 → relief\n",
      "24 → remorse\n",
      "25 → sadness\n",
      "26 → surprise\n",
      "27 → neutral\n"
     ]
    }
   ],
   "source": [
    "# Load the GoEmotions dataset\n",
    "raw_data = load_dataset(DATASET_NAME)\n",
    "print(raw_data)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Look at an example row from the training set\n",
    "example = raw_data[\"train\"][0]\n",
    "for key, value in example.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "# Hugging Face datasets carry a typed schema in .features.\n",
    "# This tells us what each column means and how labels are encoded.\n",
    "print(\"=== Dataset features schema ===\")\n",
    "dataset_features_schema = raw_data[\"train\"].features\n",
    "for key, value in dataset_features_schema.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "# The \"labels\" column is stored as a Sequence of ClassLabel objects.\n",
    "# Each example contains a list of label IDs, and each ID maps to a human-readable label name.\n",
    "labels_column_schema = dataset_features_schema[\"labels\"]\n",
    "label_id_schema = labels_column_schema.feature  # This is the ClassLabel mapping\n",
    "\n",
    "print(\"Labels column schema (list of IDs):\", labels_column_schema)\n",
    "print(\"Label ID schema (ID to name mapping):\", label_id_schema)\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "# Get number of labels and label names from the schema\n",
    "label_names = label_id_schema.names\n",
    "num_labels = label_id_schema.num_classes\n",
    "\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(\"Label names:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    print(f\"{i:2d} → {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea3a845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5427/5427 [00:00<00:00, 57458.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized columns: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Tokenized label shape: (28,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_build_multihot(batch):\n",
    "    \"\"\"\n",
    "    Converts a batch of examples into:\n",
    "      - tokenized inputs: input_ids, attention_mask\n",
    "      - multi-label targets: labels as multi-hot encoding vectors [B, num_labels]\n",
    "\n",
    "    IMPORTANT: For BCEWithLogitsLoss, labels must be floats (0.0/1.0).\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    # If a sequence is too long, truncate to max length (context window size)\n",
    "    # If a sequence is shorter than max length, padding is done later by DataCollatorWithPadding\n",
    "    tokenized = tokenizer(\n",
    "        batch[TEXT_COL],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    # Create an empty matrix of shape: (batch_size, num_labels)\n",
    "    # We will fill this with 0s and 1s to form a multi-hot encoding.\n",
    "    multi_hot = np.zeros((len(batch[\"labels\"]), num_labels), dtype=np.float32)\n",
    "    for i, label_ids in enumerate(batch[\"labels\"]):\n",
    "        for id in label_ids:\n",
    "            multi_hot[i, id] = 1.0\n",
    "\n",
    "    tokenized[\"labels\"] = multi_hot\n",
    "    return tokenized\n",
    "\n",
    "# Apply to datasets (batched mapping)\n",
    "tokenized = raw_data.map(tokenize_and_build_multihot, batched=True)\n",
    "\n",
    "# We can drop raw columns we no longer need. Keep only model-ready fields.\n",
    "cols_to_remove = [c for c in raw_data[\"train\"].column_names if c != \"labels\"]\n",
    "\n",
    "# The tokenized dataset now contains token fields + labels; remove original \"text\" etc.\n",
    "tokenized = tokenized.remove_columns(cols_to_remove)\n",
    "\n",
    "print(\"Tokenized columns:\", tokenized[\"train\"].column_names)\n",
    "print(\"Tokenized label shape:\", np.array(tokenized[\"train\"][0][\"labels\"]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a208679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Public methods in DataCollatorWithPadding ===\n",
      "max_length\n",
      "pad_to_multiple_of\n",
      "padding\n",
      "return_tensors\n",
      "\n",
      "=== Raw examples ===\n",
      "Example 0 length: 16\n",
      "Example 1 length: 24\n",
      "\n",
      "=== After DataCollatorWithPadding ===\n",
      "labels torch.Size([2, 28]) torch.int64\n",
      "input_ids torch.Size([2, 24]) torch.int64\n",
      "token_type_ids torch.Size([2, 24]) torch.int64\n",
      "attention_mask torch.Size([2, 24]) torch.int64\n",
      "\n",
      "input_ids:\n",
      "tensor([[  101,  2026,  8837,  2833,  2003,  2505,  1045,  2134,  1005,  1056,\n",
      "          2031,  2000,  5660,  2870,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2085,  2065,  2002,  2515,  2125,  2370,  1010,  3071,  2097,\n",
      "          2228,  2002,  2015,  2383,  1037,  4756, 29082,  2007,  2111,  2612,\n",
      "          1997,  2941,  2757,   102]])\n"
     ]
    }
   ],
   "source": [
    "# A data collator is the function that turns a list of dataset rows into a single training batch.\n",
    "print(\"=== Public methods in DataCollatorWithPadding ===\")\n",
    "for name in dir(DataCollatorWithPadding):\n",
    "    if not name.startswith(\"_\"): \n",
    "        print(name)\n",
    "print()\n",
    "\n",
    "# For pedagogical purposes, we instantiate the default data collator here and see what it does\n",
    "base_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Grab two examples with different sequence lengths\n",
    "sample_examples = [\n",
    "    tokenized[\"train\"][0],\n",
    "    tokenized[\"train\"][1],\n",
    "]\n",
    "\n",
    "print(\"=== Raw examples ===\")\n",
    "print(\"Example 0 length:\", len(sample_examples[0][\"input_ids\"]))\n",
    "print(\"Example 1 length:\", len(sample_examples[1][\"input_ids\"]))\n",
    "print()\n",
    "\n",
    "# Collate them into a batch\n",
    "batch = base_collator(sample_examples)\n",
    "\n",
    "print(\"=== After DataCollatorWithPadding ===\")\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape, v.dtype)\n",
    "\n",
    "# Show padding effect\n",
    "print(\"\\ninput_ids:\")\n",
    "print(batch[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b35d738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollatorWithPadding pads each batch to the longest example in that batch.\n",
    "# For multi-label classification, we also need to ensure \n",
    "# labels are float tensors in order to be compatible with BCEWithLogitsLoss.\n",
    "# So we create a custom data collator by subclassing DataCollatorWithPadding.\n",
    "class CustomDataCollatorWithPadding(DataCollatorWithPadding):\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        # Run Hugging Face's default padding + tensorization\n",
    "        batch = super().__call__(features)\n",
    "\n",
    "        # BCEWithLogitsLoss expects float targets\n",
    "        batch[\"labels\"] = batch[\"labels\"].float()\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = CustomDataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414c493",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70936e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=28, bias=True)\n",
      ")\n",
      "\n",
      "=== Classification head ===\n",
      "Linear(in_features=768, out_features=28, bias=True)\n",
      "\n",
      "Classifier weight matrix shape: torch.Size([28, 768])\n",
      "Classifier bias shape: torch.Size([28])\n",
      "\n",
      "Total parameters: 109,503,772\n",
      "Classifier head parameters: 21,532\n",
      "Fraction in classifier head: 0.0197%\n"
     ]
    }
   ],
   "source": [
    "# AutoModelForSequenceClassification adds a linear, task-specific classification head on top of BERT.\n",
    "# The head is a single fully connected linear layer that maps the [CLS] to num_labels logits.\n",
    "# Its weights are randomly initialized.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\", # problem_type=\"multi_label_classification\" tells HF to use BCEWithLogitsLoss.\n",
    ").to(device)\n",
    "\n",
    "# Look at the model architecture\n",
    "print(model)\n",
    "\n",
    "# View the classification head specifically\n",
    "print(\"\\n=== Classification head ===\")\n",
    "print(model.classifier)\n",
    "\n",
    "# We can also access the classification head parameters\n",
    "W = model.classifier.weight\n",
    "b = model.classifier.bias\n",
    "\n",
    "print(\"\\nClassifier weight matrix shape:\", W.shape)\n",
    "print(\"Classifier bias shape:\", b.shape)\n",
    "\n",
    "# How many parameters does the model have?\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "head_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Classifier head parameters: {head_params:,}\")\n",
    "print(f\"Fraction in classifier head: {head_params / total_params:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce34c0f",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95fae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for a multi-label classification model.\n",
    "\n",
    "    This function converts model logits into binary label predictions using a\n",
    "    sigmoid + threshold, then computes three complementary metrics:\n",
    "\n",
    "    - f1_micro: Treats every (example, label) decision as an independent binary\n",
    "                prediction and aggregates them globally. \n",
    "\n",
    "    - f1_macro: Computes F1 separately for each label and then averages them. \n",
    "\n",
    "    - exact_match: The fraction of examples for which the model predicted \n",
    "                   *all* labels correctly.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred  # logits: (N, num_labels), labels: (N, num_labels)\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))                 # sigmoid\n",
    "    preds = (probs >= THRESHOLD).astype(np.int32)     # multi-label predictions\n",
    "\n",
    "    labels = labels.astype(np.int32)                  # ground truth multi-hot\n",
    "\n",
    "    micro = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    exact_match = float(np.mean(np.all(preds == labels, axis=1)))\n",
    "\n",
    "    return {\"f1_micro\": micro, \"f1_macro\": macro, \"exact_match\": exact_match}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70af84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check batch shapes:\n",
      " input_ids: torch.Size([16, 36])\n",
      " attention_mask: torch.Size([16, 36])\n",
      " labels: torch.Size([16, 28]) torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npatn\\AppData\\Local\\Temp\\ipykernel_35868\\3660858513.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments - super easy with Transformers library!\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert-goemotions-multilabel\", # Where checkpoints, logs, and trainer outputs are saved\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",  # When to run evaluation\n",
    "    save_strategy=\"epoch\",  # When to save model checkpoints\n",
    "    load_best_model_at_end=True, # After training, reload the checkpoint with the best evaluation metric\n",
    "    metric_for_best_model=\"f1_micro\",  # Metric used to determine which checkpoint is \"best\"\n",
    "    greater_is_better=True,  # Indicates if larger metric values are better (True for F1)\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\", # Disable integration with external experiment trackers (e.g., Weights & Biases)\n",
    "    fp16=torch.cuda.is_available(), # Mixed-precision (FP16) to reduce memory\n",
    ")\n",
    "\n",
    "# Trainer: combines model, args, datasets, tokenizer, data collator, metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Optional but useful sanity check\n",
    "batch = next(iter(trainer.get_train_dataloader()))\n",
    "print(\"Sanity check batch shapes:\")\n",
    "print(\" input_ids:\", batch[\"input_ids\"].shape)\n",
    "print(\" attention_mask:\", batch[\"attention_mask\"].shape)\n",
    "print(\" labels:\", batch[\"labels\"].shape, batch[\"labels\"].dtype)\n",
    "# Expect labels dtype float32 for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9032b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27140' max='27140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27140/27140 37:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Exact Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>0.090875</td>\n",
       "      <td>0.535611</td>\n",
       "      <td>0.288413</td>\n",
       "      <td>0.407114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.084159</td>\n",
       "      <td>0.573554</td>\n",
       "      <td>0.411853</td>\n",
       "      <td>0.452820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.084958</td>\n",
       "      <td>0.577879</td>\n",
       "      <td>0.436225</td>\n",
       "      <td>0.461482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.090412</td>\n",
       "      <td>0.579297</td>\n",
       "      <td>0.461265</td>\n",
       "      <td>0.466826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.097697</td>\n",
       "      <td>0.572647</td>\n",
       "      <td>0.460999</td>\n",
       "      <td>0.462403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.104581</td>\n",
       "      <td>0.571813</td>\n",
       "      <td>0.470760</td>\n",
       "      <td>0.463878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.111031</td>\n",
       "      <td>0.557675</td>\n",
       "      <td>0.466242</td>\n",
       "      <td>0.449687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.116304</td>\n",
       "      <td>0.568578</td>\n",
       "      <td>0.476703</td>\n",
       "      <td>0.455031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.119289</td>\n",
       "      <td>0.564945</td>\n",
       "      <td>0.479297</td>\n",
       "      <td>0.449134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.120857</td>\n",
       "      <td>0.564286</td>\n",
       "      <td>0.474608</td>\n",
       "      <td>0.449134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.09041234850883484, 'eval_f1_micro': 0.579297079909075, 'eval_f1_macro': 0.46126483391374357, 'eval_exact_match': 0.4668263914485809, 'eval_runtime': 7.1503, 'eval_samples_per_second': 758.852, 'eval_steps_per_second': 23.775, 'epoch': 10.0}\n",
      "Saved to: bert-goemotions-multilabel-finetuned\n"
     ]
    }
   ],
   "source": [
    "# Train, evaluate, and save model weights + architecture\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Validation metrics:\", metrics)\n",
    "\n",
    "save_dir = \"bert-goemotions-multilabel-finetuned\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(\"Saved to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d979a",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80426fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: I can't believe you did that. I'm furious.\n",
      "Top-K: [('anger', 0.8766343593597412), ('annoyance', 0.193451926112175), ('neutral', 0.022328782826662064)]\n",
      "Above threshold: [('anger', 0.8766343593597412)]\n",
      "\n",
      "TEXT: That was so kind of you, thank you so much!\n",
      "Top-K: [('gratitude', 0.9874721169471741), ('admiration', 0.32874488830566406), ('approval', 0.012241275049746037)]\n",
      "Above threshold: [('gratitude', 0.9874721169471741)]\n",
      "\n",
      "TEXT: I'm a bit nervous about tomorrow, but also sort of excited.\n",
      "Top-K: [('excitement', 0.442516028881073), ('nervousness', 0.2485341727733612), ('fear', 0.06669065356254578)]\n",
      "Above threshold: []\n"
     ]
    }
   ],
   "source": [
    "# Inference: predict emotions for new text\n",
    "# Set to evaluation mode: disables training-only behavior like dropout and batch norm updates.\n",
    "model.eval()\n",
    "\n",
    "def predict_emotions(texts, top_k=5, threshold=THRESHOLD):\n",
    "    \"\"\"\n",
    "    In classification inference:\n",
    "      text -> tokenize -> logits -> sigmoid -> label probabilities -> choose labels\n",
    "\n",
    "    Args:\n",
    "      texts (List[str]): input texts to classify\n",
    "      top_k (int): number of highest-probability labels to return\n",
    "      threshold (float): probability cutoff for selecting labels\n",
    "\n",
    "    Returns:\n",
    "      List[dict]: one result per input text, containing:\n",
    "        - original text\n",
    "        - top-k predicted labels\n",
    "        - labels whose probability exceeds the threshold\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    # Forward pass through the model: input is tokenized texts\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits  # [B, num_labels]\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    # Post-processing results into readable format\n",
    "    results = []\n",
    "    for text, p in zip(texts, probs):\n",
    "        top_idx = np.argsort(-p)[:top_k] # Sort label indices by prob (descending) and keep top_k\n",
    "        top = [(label_names[i], float(p[i])) for i in top_idx] # Convert indices into (label_name, probability) pairs\n",
    "\n",
    "        # Choose labels where prob exceeds threshold\n",
    "        chosen_idx = np.where(p >= threshold)[0] \n",
    "        chosen = [(label_names[i], float(p[i])) for i in chosen_idx]\n",
    "        chosen = sorted(chosen, key=lambda x: -x[1]) # Sort selected labels by probab (highest first)\n",
    "\n",
    "        results.append({\"text\": text, \"top_k\": top, \"above_threshold\": chosen})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "demo_texts = [\n",
    "    \"I can't believe you did that. I'm furious.\",\n",
    "    \"That was so kind of you, thank you so much!\",\n",
    "    \"I'm a bit nervous about tomorrow, but also sort of excited.\",\n",
    "]\n",
    "\n",
    "for r in predict_emotions(demo_texts, top_k=3, threshold=0.5):\n",
    "    print(\"\\nTEXT:\", r[\"text\"])\n",
    "    print(\"Top-K:\", r[\"top_k\"])\n",
    "    print(\"Above threshold:\", r[\"above_threshold\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6c9b5",
   "metadata": {},
   "source": [
    "## Compare: Evaluation Metrics for Pre-trained vs Fine-Tuned Model\n",
    "\n",
    "A pre-trained BERT model with a randomly initialized classification head will usually score near chance on F1, because the head is untrained. Let's check and compare it against our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9d05aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\npatn\\AppData\\Local\\Temp\\ipykernel_35868\\2039803065.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  base_trainer = Trainer(\n",
      "C:\\Users\\npatn\\AppData\\Local\\Temp\\ipykernel_35868\\2039803065.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  ft_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline (pretrained) ===\n",
      "eval_loss: 0.7067374587059021\n",
      "eval_model_preparation_time: 0.0073\n",
      "eval_f1_micro: 0.0923913686077446\n",
      "eval_f1_macro: 0.06380048823264672\n",
      "eval_exact_match: 0.0\n",
      "eval_runtime: 7.2064\n",
      "eval_samples_per_second: 752.937\n",
      "eval_steps_per_second: 23.59\n",
      "\n",
      "=== Fine-tuned ===\n",
      "eval_loss: 0.09041234850883484\n",
      "eval_model_preparation_time: 0.0083\n",
      "eval_f1_micro: 0.579297079909075\n",
      "eval_f1_macro: 0.46126483391374357\n",
      "eval_exact_match: 0.4668263914485809\n",
      "eval_runtime: 7.1416\n",
      "eval_samples_per_second: 759.774\n",
      "eval_steps_per_second: 23.804\n",
      "\n",
      "=== Difference (fine-tuned - baseline) ===\n",
      "eval_loss: -0.6163\n",
      "eval_model_preparation_time: +0.0010\n",
      "eval_f1_micro: +0.4869\n",
      "eval_f1_macro: +0.3975\n",
      "eval_exact_match: +0.4668\n",
      "eval_runtime: -0.0648\n",
      "eval_samples_per_second: +6.8370\n",
      "eval_steps_per_second: +0.2140\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Reload a new, Base Model to compare our Fine-Tuned Model against\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\", \n",
    ").to(device)\n",
    "\n",
    "# Corresponding Trainer for the Base Model\n",
    "base_trainer = Trainer(\n",
    "    model=base_model, # Pre-trained weights\n",
    "    args=args,  # Same TrainingArguments as before\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Create a new Trainer object for our Fine-Tuned Model (safer than re-using the one used during Trainer)\n",
    "ft_trainer = Trainer(\n",
    "    model=model,  # Recall: this is our model fine-tuned on GoEmotions!\n",
    "    args=args, # Same TrainingArguments as before\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Use the new Trainers to evaluate each respective model\n",
    "base_metrics = base_trainer.evaluate()\n",
    "ft_metrics = ft_trainer.evaluate()\n",
    "\n",
    "print(\"\\n=== Baseline (pretrained) ===\")\n",
    "for k, v in base_metrics.items():\n",
    "    if k.startswith(\"eval_\"):\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Fine-tuned ===\")\n",
    "for k, v in ft_metrics.items():\n",
    "    if k.startswith(\"eval_\"):\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Difference (fine-tuned - baseline) ===\")\n",
    "for k in ft_metrics:\n",
    "    if k.startswith(\"eval_\") and isinstance(ft_metrics[k], (int, float)):\n",
    "        b = base_metrics.get(k, None)\n",
    "        if isinstance(b, (int, float)):\n",
    "            print(f\"{k}: {ft_metrics[k] - b:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
