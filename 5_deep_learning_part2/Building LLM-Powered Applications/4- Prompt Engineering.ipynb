{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7b6ef4",
   "metadata": {},
   "source": [
    "# Basics of Prompt Engineering\n",
    "\n",
    "This notebook was created by Natasha Patnaik for MIT 15.S60 - Computing in Optimization and Statistics. \n",
    "\n",
    "**Last Updated**: January 2026."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdae8b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\npatn\\OneDrive\\Documents\\15.S60_2026\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "c:\\Users\\npatn\\OneDrive\\Documents\\15.S60_2026\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "OLLAMA_MODEL = \"llama3.1:8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec21dd",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Without modifying a general-purpose language model's parameters (i.e. - no more training updates), can we adapt it to perform better on a specific downstream task, purely at inference time?\n",
    "\n",
    "_Prompt engineering_ tries to address this question by carefully structuring the input prompt to be compatible with how the model was originally pre-trained. Since generative language models are typically pre-trained for the next-token prediction task, we want to elicit better outputs by formatting our inputs as clear **task completion** problems.\n",
    "\n",
    "More generally, _prompt engineering_ is a relatviely newer discipline about designing a collection of tips, tricks, and techniques for typically getting better performance in practice. Importantly, these prompt design principles are not guaranteed to work in all cases. They are empirical, task-dependent, and heuristic in nature, rather than a strict set of prescriptive rules that must be adhered to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c3d9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no \"rs\" in a strawberry. A strawberry is a type of fruit, and it doesn't have any monetary value or currency associated with it.\n",
      "\n",
      "If you meant to ask something else, please feel free to clarify!\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the ChatOllama class, specifying our model.\n",
    "llm = ChatOllama(model=OLLAMA_MODEL, temperature=0.2)\n",
    "\n",
    "# A standard prompt as the baseline\n",
    "# We'll try and see if we can get better output with some prompt engineering....\n",
    "baseline_prompt = \"How many rs are there in strawberry.\"\n",
    "response = llm.invoke(baseline_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbefe47d",
   "metadata": {},
   "source": [
    "## In-context Learning\n",
    "\n",
    "Here's the paper from the OpenAI Team that looked at few-shot learning with GPT-3: https://arxiv.org/abs/2005.14165. \n",
    "\n",
    "### Few-shot, One-shot, and Zero-shot Prompting\n",
    "\n",
    "Suppose you have a natural language description of the task you would like the language model to complete. You put this description in the prompt. If your prompt contains only the task description, this is called zero-shot prompting.\n",
    "\n",
    "If you additionally include one correctly completed example in the prompt, this is referred to as one-shot prompting. Including multiple examples is known as few-shot prompting.\n",
    "\n",
    "By providing examples of the task being completed correctly in the prompt itself, the language model can generate output that is conditioned on these examples in the input. This pehnomenon is called **in-context learning** - which becomes increasingly pronounced as language models are scaled to larger sizes.\n",
    "\n",
    "This approach requires orders of magnitude fewer labeled pairs than supervised fine-tuning! Moreover, this inference-time approach doesn't update the model's parameters or architecture in any way, thus enabling it to retain general-purpose capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c19cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot answer:\n",
      "The letter \"r\" appears 3 times in the word \"strawberry\".\n",
      "\n",
      "One-shot answer:\n",
      "Let's count the number of times the letter \"r\" appears in the phrase \"strawberry\":\n",
      "\n",
      "1. str- (1 \"r\")\n",
      "2. a-w-b-e-r-y (2 more \"r\"s)\n",
      "\n",
      "So, the total number of times the letter \"r\" appears in the phrase \"strawberry\" is: 3\n",
      "\n",
      "Few-shot answer:\n",
      "Let's count the number of times the letter 'r' appears in each phrase:\n",
      "\n",
      "* strawberry: 3\n"
     ]
    }
   ],
   "source": [
    "# ZERO-SHOT EXAMPLE\n",
    "zero_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Count the number of times the letter 'r' appears in the following phrase:\n",
    "\n",
    "{phrase}\n",
    "\"\"\",\n",
    "    input_variables=[\"phrase\"],\n",
    ")\n",
    "response = llm.invoke(zero_shot_prompt.format(phrase=\"strawberry\"))\n",
    "print(\"Zero-shot answer:\")\n",
    "print(response.content)\n",
    "print(\"\")\n",
    "\n",
    "# ONE-SHOT EXAMPLE\n",
    "one_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Count the number of times the letter 'r' appears in the following phrases:\n",
    "\n",
    "* red rover => 3\n",
    "\n",
    "{new_phrase} =>\n",
    "\"\"\",\n",
    "    input_variables=[\"new_phrase\"],\n",
    ")\n",
    "\n",
    "response = llm.invoke(one_shot_prompt.format(new_phrase=\"strawberry\"))\n",
    "print(\"One-shot answer:\")\n",
    "print(response.content)\n",
    "print(\"\")\n",
    "\n",
    "# FEW-SHOT EXAMPLE\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Count the number of times the letter 'r' appears in the following phrases:\n",
    "\n",
    "* red rover => 3\n",
    "* roaring river => 4\n",
    "* green => 1\n",
    "* red riding hood => 2\n",
    "* ferry => 2\n",
    "* raspberry cream => 4\n",
    "\n",
    "* {new_phrase} =>\n",
    "\"\"\",\n",
    "    input_variables=[\"new_phrase\"],\n",
    ")\n",
    "\n",
    "response = llm.invoke(few_shot_prompt.format(new_phrase=\"strawberry\"))\n",
    "print(\"Few-shot answer:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9108921",
   "metadata": {},
   "source": [
    "## Role Indicators\n",
    "\n",
    "Within the prompt template, you can mark certain sections as belonging to a specific \"role\", such as an assistant, user, or system instructions. This imposes more structure on conversations, allowing us to easily distinguish between high-level instructions and specific user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3746a85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's count the number of times the letter 'r' appears in each character of the phrase \"strawberry\":\n",
      "\n",
      "s-t-r-a-w-b-e-r-r-y\n",
      "\n",
      "The letter 'r' appears 3 times.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a precise assistant that follows patterns. You look at every character in the phrase.\"),\n",
    "     (\"user\",\n",
    "            \"\"\"\n",
    "Count the number of times the letter 'r' appears in the following phrases:\n",
    "\n",
    "red rover => 3\n",
    "roaring river => 4\n",
    "green => 1\n",
    "red riding hood => 2\n",
    "ferry => 2\n",
    "raspberry cream => 4\n",
    "\n",
    "strawberry =>\n",
    "\"\"\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = prompt.format_messages()\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197b564",
   "metadata": {},
   "source": [
    "## Chain of Thought (CoT) Prompting\n",
    "\n",
    "Here's the paper from Google Research: https://arxiv.org/pdf/2201.11903.\n",
    "\n",
    "When providing examples of succesfully completed tasks in the prompt, include the intermediate steps used to arrive at the correct/ desired answer. By structuring the examples to expicitly show this reasoning trace, the language model will use this context to generate answers that break down the larger task into simpler steps. This makes generating the correct/ desired next token for each intermediate step more likley, thus resulting in a improved final answer.\n",
    "\n",
    "You don't necessarily need to include full examples in a few-shot or one-shot structure to leverage the CoT format. You can also take a zero-shot approach without any solved examples, but simply add a \"_break down the task step-by-step to arrive at the final answer_\" instruction line to encourage intermediate reasoning.\n",
    "\n",
    "The [original paper](https://arxiv.org/pdf/2201.11903) also states: \"*Notably, chain-of-thought reasoning\n",
    "is an emergent ability of increasing model scale*\". This means the benefits of CoT prompting over standard prompting become more apparent for larger language models (i.e. - more billions of parameters).\n",
    "\n",
    "A nice side-benefit of this approach is that you can also view the reasoning trace to see the steps leading up to the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0df2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided reasoning, I will count the number of times the letter 'r' appears in each phrase:\n",
      "\n",
      "1. Phrase: red rover\n",
      "   Total = 3 (as calculated)\n",
      "\n",
      "2. Phrase: ferry\n",
      "   Total = 2 (as calculated)\n",
      "\n",
      "3. Phrase: green\n",
      "   Total = 1 (as calculated)\n",
      "\n",
      "4. Phrase: raspberry cream\n",
      "   - r appears in \"r-a-s-p-b-e-r-r-y\" thrice.\n",
      "   - r appears once in \"c-r-e-a-m\"\n",
      "   Total = 4 (as calculated)\n",
      "\n",
      "5. Phrase: strawberry\n",
      "   Reasoning:\n",
      "   - r appears in \"s-t-r-w-b-e-r-r-y\" thrice.\n",
      "\n",
      "Total for the phrase \"strawberry\": 3\n"
     ]
    }
   ],
   "source": [
    "cot_few_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Count the number of times the letter 'r' appears in the following phrases:\n",
    "\n",
    "Phrase: red rover\n",
    "Reasoning:\n",
    "- r appears in \"r-e-d\" once.\n",
    "- r appears in \"r-o-v-e-r\" twice.\n",
    "Total = 3\n",
    "\n",
    "Phrase: ferry\n",
    "Reasoning:\n",
    "- r appears in \"f-e-r-r-y\" twice.\n",
    "Total = 2\n",
    "\n",
    "Phrase: green\n",
    "Reasoning:\n",
    "- r appears in \"g-r-e-e-n\" once.\n",
    "Total = 1\n",
    "\n",
    "Phrase: raspberry cream\n",
    "Reasoning:\n",
    "- r appears in \"r-a-s-p-b-e-r-r-y\" thrice.\n",
    "- r appears once in \"c-r-e-a-m\" once.\n",
    "Total = 4\n",
    "\n",
    "Phrase: {new_phrase}\n",
    "Reasoning:\n",
    "\"\"\",\n",
    "    input_variables=[\"new_phrase\"],\n",
    ")\n",
    "\n",
    "response = llm.invoke(cot_few_shot_prompt.format(new_phrase=\"strawberry\"))\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f27b8e",
   "metadata": {},
   "source": [
    "## Iterate! Improve your prompts over time.\n",
    "\n",
    "It helps to experiment with your prompt to see what works best for your application. If you empirically test out different versions of your prompt for the task at hand, and track the outcomes over time, you'll get a sense of what works well in practice. \n",
    "\n",
    "Being as specific as possible and leveraging a consistent template/ well-defined structure can help. Additionally, explicit instructions that require some default output or pre-set option if the task is too difficult can help mitigate the risk of hallucination."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
